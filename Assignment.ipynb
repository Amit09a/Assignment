{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Q: What is Simple Linear Regression?\n",
        "- Simple Linear Regression is a statistical method used to model and predict a dependent variable\n",
        "ğ‘Œ\n",
        "Y using one independent variable\n",
        "ğ‘‹\n",
        "X by fitting a straight-line equation\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "Y=mX+c.\n",
        "\n",
        "Q: What are the key assumptions of Simple Linear Regression?\n",
        "- The key assumptions are linearity (relationship is linear), independence of errors, homoscedasticity (constant error variance), approximate normality of residuals (mainly for inference), and absence of extreme outliers/influential points.\n",
        "\n",
        "Q: What does the coefficient\n",
        "ğ‘š\n",
        "m represent in the equation\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ‘š\n",
        "ğ‘‹\n",
        "+\n",
        "ğ‘\n",
        "Y=mX+c?\n",
        "-  \n",
        "ğ‘š\n",
        "m is the slope; it represents the expected change in\n",
        "ğ‘Œ\n",
        "Y for a 1-unit increase in\n",
        "ğ‘‹\n",
        "X.\n",
        "\n",
        "Q: What does the intercept\n",
        "ğ‘\n",
        "c represent in the equation ğ‘Œ= ğ‘šğ‘‹+ğ‘Y=mX+c?\n",
        "-\n",
        "ğ‘\n",
        "c is the intercept; it represents the predicted value of\n",
        "ğ‘Œ\n",
        "Y when\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0 (a baseline value, meaningful only if\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0 is sensible in context).\n",
        "\n",
        "Q: How do we calculate the slope\n",
        "ğ‘š\n",
        "m in Simple Linear Regression?\n",
        "-  \n",
        "ğ‘š\n",
        "=\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "Ë‰\n",
        ")\n",
        "(\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "Ë‰\n",
        ")\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘¥\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¥\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "=\n",
        "Cov\n",
        "(\n",
        "ğ‘‹\n",
        ",\n",
        "ğ‘Œ\n",
        ")\n",
        "Var\n",
        "(\n",
        "ğ‘‹\n",
        ")\n",
        "m=\n",
        "âˆ‘(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’\n",
        "x\n",
        "Ë‰\n",
        ")\n",
        "2\n",
        "âˆ‘(x\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’\n",
        "x\n",
        "Ë‰\n",
        ")(y\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’\n",
        "y\n",
        "Ë‰\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "\tâ€‹\n",
        "\n",
        "=\n",
        "Var(X)\n",
        "Cov(X,Y)\n",
        "\tâ€‹\n",
        "\n",
        ".\n",
        "\n",
        "Q: What is the purpose of the least squares method in Simple Linear Regression?\n",
        "-  It finds\n",
        "ğ‘š\n",
        "m and\n",
        "ğ‘\n",
        "c that minimize the sum of squared residuals\n",
        "âˆ‘\n",
        "(\n",
        "ğ‘¦\n",
        "ğ‘–\n",
        "âˆ’\n",
        "ğ‘¦\n",
        "^\n",
        "ğ‘–\n",
        ")\n",
        "2\n",
        "âˆ‘(y\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        "âˆ’\n",
        "y\n",
        "^\n",
        "\tâ€‹\n",
        "\n",
        "i\n",
        "\tâ€‹\n",
        "\n",
        ")\n",
        "2\n",
        ", giving the best-fitting line under that criterion.\n",
        "\n",
        "Q: How is the coefficient of determination (RÂ²) interpreted in Simple Linear Regression?\n",
        "-  \n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " is the proportion of variance in\n",
        "ğ‘Œ\n",
        "Y explained by the model using\n",
        "ğ‘‹\n",
        "X; higher\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " indicates the line explains more variability in\n",
        "ğ‘Œ\n",
        "Y.\n",
        "\n",
        "Q: What is Multiple Linear Regression?\n",
        "-  Multiple Linear Regression models\n",
        "ğ‘Œ\n",
        "Y using two or more predictors:\n",
        "ğ‘Œ\n",
        "=\n",
        "ğ›½\n",
        "0\n",
        "+\n",
        "ğ›½\n",
        "1\n",
        "ğ‘‹\n",
        "1\n",
        "+\n",
        "ğ›½\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ›½\n",
        "ğ‘\n",
        "ğ‘‹\n",
        "ğ‘\n",
        "+\n",
        "ğœ€\n",
        "Y=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "+â‹¯+Î²\n",
        "p\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "p\n",
        "\tâ€‹\n",
        "\n",
        "+Îµ.\n",
        "\n",
        "Q: What is the main difference between Simple and Multiple Linear Regression?\n",
        "-  Simple Linear Regression uses one predictor; Multiple Linear Regression uses multiple predictors, and each coefficient is interpreted â€œholding other variables constant.â€\n",
        "\n",
        "Q: What are the key assumptions of Multiple Linear Regression?\n",
        "-  Linearity in parameters, independence of errors, homoscedasticity, approximate normality of residuals (for inference), no perfect multicollinearity, and correct model specification (relevant variables/functional form).\n",
        "\n",
        "Q: What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
        "- Heteroscedasticity means non-constant error variance; coefficients may remain unbiased, but standard errors can be wrong, leading to unreliable p-values and confidence intervals.\n",
        "\n",
        "Q: How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
        "- Remove/merge highly correlated predictors, use regularization (Ridge/Lasso), apply dimensionality reduction (e.g., PCA), or redesign features based on domain knowledge.\n",
        "\n",
        "Q: What are some common techniques for transforming categorical variables for use in regression models?\n",
        "- One-hot/dummy encoding, ordinal encoding (if order matters), and target encoding (use carefully to avoid leakage).\n",
        "\n",
        "Q: What is the role of interaction terms in Multiple Linear Regression?\n",
        "- Interaction terms (e.g.,\n",
        "ğ‘‹\n",
        "1\n",
        "ğ‘‹\n",
        "2\n",
        "X\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        ") model situations where the effect of one predictor on\n",
        "ğ‘Œ\n",
        "Y depends on the level of another predictor.\n",
        "\n",
        "Q: How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
        "- In Simple LR, intercept is\n",
        "ğ‘Œ\n",
        "Y at\n",
        "ğ‘‹\n",
        "=\n",
        "0\n",
        "X=0; in Multiple LR, it is\n",
        "ğ‘Œ\n",
        "Y when all predictors are zero simultaneously, which is often less realistic unless variables are centered.\n",
        "\n",
        "Q: What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
        "- The slope/coefficients quantify direction and magnitude of effect; larger magnitude means predictions change more for a 1-unit change in that predictor (with others held constant in MLR).\n",
        "\n",
        "Q: How does the intercept in a regression model provide context for the relationship between variables?\n",
        "- The intercept sets the baseline level of\n",
        "ğ‘Œ\n",
        "Y at the reference point (predictors at zero), anchoring the prediction equation and positioning the fitted line/hyperplane.\n",
        "\n",
        "Q: What are the limitations of using RÂ² as a sole measure of model performance?\n",
        "- It can increase just by adding predictors (overfitting risk), does not measure generalization, and does not reveal assumption violations or error magnitude; adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        ", CV metrics, and residual diagnostics are needed.\n",
        "\n",
        "Q: How would you interpret a large standard error for a regression coefficient?\n",
        "- It indicates high uncertainty in the coefficient estimate (often due to noise, small sample, or multicollinearity), leading to wide confidence intervals and weaker statistical significance.\n",
        "\n",
        "Q: How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
        "- A funnel/cone pattern (residual spread changes with fitted values) suggests heteroscedasticity; addressing it is important because it invalidates standard errors and inference unless corrected (robust SEs, transforms, WLS).\n",
        "\n",
        "Q: What does it mean if a Multiple Linear Regression model has a high RÂ² but low adjusted RÂ²?\n",
        "- It suggests extra predictors are not truly contributing; adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        " penalizes unnecessary variables, so the model may be overfit or too complex.\n",
        "\n",
        "Q: Why is it important to scale variables in Multiple Linear Regression?\n",
        "- Scaling helps numerical stability and is critical when using regularization (Ridge/Lasso) so penalties treat predictors fairly; it also helps compare coefficient effects when units differ.\n",
        "\n",
        "Q: What is polynomial regression?\n",
        "- Polynomial regression models\n",
        "ğ‘Œ\n",
        "Y as a polynomial function of\n",
        "ğ‘‹\n",
        "X (e.g., includes\n",
        "ğ‘‹\n",
        "2\n",
        ",\n",
        "ğ‘‹\n",
        "3\n",
        "X\n",
        "2\n",
        ",X\n",
        "3\n",
        "), while remaining linear in coefficients.\n",
        "\n",
        "Q: How does polynomial regression differ from linear regression?\n",
        "- Linear regression fits a straight line; polynomial regression adds higher-degree terms so it can fit curved relationships.\n",
        "\n",
        "Q: When is polynomial regression used?\n",
        "- When data shows a clear non-linear (curved) trend that a straight line cannot capture, but a parametric polynomial form is adequate within the observed range.\n",
        "\n",
        "Q: What is the general equation for polynomial regression?\n",
        "-\n",
        "ğ‘Œ\n",
        "^\n",
        "=\n",
        "ğ›½\n",
        "0\n",
        "+\n",
        "ğ›½\n",
        "1\n",
        "ğ‘‹\n",
        "+\n",
        "ğ›½\n",
        "2\n",
        "ğ‘‹\n",
        "2\n",
        "+\n",
        "â‹¯\n",
        "+\n",
        "ğ›½\n",
        "ğ‘‘\n",
        "ğ‘‹\n",
        "ğ‘‘\n",
        "Y\n",
        "^\n",
        "=Î²\n",
        "0\n",
        "\tâ€‹\n",
        "\n",
        "+Î²\n",
        "1\n",
        "\tâ€‹\n",
        "\n",
        "X+Î²\n",
        "2\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "2\n",
        "+â‹¯+Î²\n",
        "d\n",
        "\tâ€‹\n",
        "\n",
        "X\n",
        "d\n",
        ".\n",
        "\n",
        "Q: Can polynomial regression be applied to multiple variables?\n",
        "- Yes, by adding polynomial terms for each predictor and optionally interaction/cross terms (feature expansion).\n",
        "\n",
        "Q: What are the limitations of polynomial regression?\n",
        "- It can overfit with high degree, extrapolates poorly beyond data, may create multicollinearity among polynomial terms, and becomes less interpretable as complexity increases.\n",
        "\n",
        "Q: What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
        "- Cross-validation, validation-set RMSE/MAE, adjusted\n",
        "ğ‘…\n",
        "2\n",
        "R\n",
        "2\n",
        ", AIC/BIC, and checking residual plots for patterns.\n",
        "\n",
        "Q: Why is visualization important in polynomial regression?\n",
        "- It helps confirm curvature, detect overfitting/underfitting, and verify the fitted curve behaves sensibly compared to observed data and residuals.\n",
        "\n",
        "Q: How is polynomial regression implemented in Python?\n",
        "- Create polynomial features (e.g., sklearn.preprocessing.PolynomialFeatures), fit a linear model (LinearRegression or Ridge/Lasso), then evaluate using train/test split or cross-validation with metrics like RMSE/MAE."
      ],
      "metadata": {
        "id": "c_WvDI0aDNQX"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qlWZvpkPDIGK"
      },
      "outputs": [],
      "source": []
    }
  ]
}