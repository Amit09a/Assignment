{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1: What is a Decision Tree, and how does it work in the context of classification?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "A Decision Tree is a supervised machine learning algorithm used for classification and regression tasks. In classification, it works by recursively splitting the dataset based on feature values to create branches that lead to a class label.\n",
        "\n",
        "- Each internal node represents a decision based on a feature, each branch represents an outcome of that decision, and each leaf node represents a predicted class. The tree selects splits using impurity measures such as Gini Impurity or Entropy to maximize class separation. During prediction, a data point traverses the tree from root to leaf following the decision rules, and the class at the leaf node is assigned as the output.\n",
        "\n",
        "Question 2: Explain the concepts of Gini Impurity and Entropy as impurity measures. How do they impact the splits in a Decision Tree?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Gini Impurity and Entropy are metrics used to measure the impurity or disorder of a dataset at a node.\n",
        "\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen sample if labels were assigned randomly according to class distribution.\n",
        "\n",
        "Entropy measures the amount of uncertainty or information disorder in the dataset.\n",
        "\n",
        "Lower impurity values indicate purer nodes. During tree construction, the algorithm evaluates all possible splits and selects the one that results in the maximum reduction in impurity. Gini tends to be faster computationally, while Entropy provides a more information-theoretic interpretation.\n",
        "\n",
        "Question 3: What is the difference between Pre-Pruning and Post-Pruning in Decision Trees? Give one practical advantage of using each.\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Pre-Pruning stops the tree from growing further during training by setting constraints such as maximum depth or minimum samples per split.\n",
        "Advantage: Reduces overfitting and training time.\n",
        "\n",
        "Post-Pruning allows the tree to grow fully and then removes unnecessary branches after training.\n",
        "Advantage: Often results in better generalization since pruning decisions are based on validation performance.\n",
        "\n",
        "Question 4: What is Information Gain in Decision Trees, and why is it important for choosing the best split?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Information Gain measures the reduction in entropy achieved after splitting a dataset based on a feature. It quantifies how much information a feature provides about the target variable.\n",
        "\n",
        "The feature with the highest Information Gain is chosen for splitting because it best separates the data into distinct classes. This ensures that each split improves the modelâ€™s ability to classify data correctly.\n",
        "\n",
        "Question 5: What are some common real-world applications of Decision Trees, and what are their main advantages and limitations?\n",
        "\n",
        "- Answer:\n",
        "\n",
        "Applications:\n",
        "\n",
        "Medical diagnosis\n",
        "\n",
        "Credit risk assessment\n",
        "\n",
        "Fraud detection\n",
        "\n",
        "Customer churn prediction\n",
        "\n",
        "Recommendation systems\n",
        "\n",
        "- Advantages:\n",
        "\n",
        "Easy to interpret and visualize\n",
        "\n",
        "Works with both numerical and categorical data\n",
        "\n",
        "Requires minimal data preprocessing\n",
        "\n",
        "- Limitations:\n",
        "\n",
        "Prone to overfitting\n",
        "\n",
        "Sensitive to small data changes\n",
        "\n",
        "Can create biased trees with imbalanced datasets"
      ],
      "metadata": {
        "id": "FOge9uiKDMzr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "62pcAFF-DJKl",
        "outputId": "10c1ecfb-3ece-4bdb-8812-8abdc28d7491"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Feature Importances: [0.         0.01667014 0.90614339 0.07718647]\n"
          ]
        }
      ],
      "source": [
        "# Question 6: Write a Python program to load the Iris Dataset, train a Decision Tree Classifier using the Gini criterion, and print accuracy and feature importances.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Output\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 7: Write a Python program to train a Decision Tree with max_depth=3 and compare its accuracy with a fully-grown tree.\n",
        "\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Fully grown tree\n",
        "full_tree = DecisionTreeClassifier(random_state=42)\n",
        "full_tree.fit(X_train, y_train)\n",
        "full_acc = accuracy_score(y_test, full_tree.predict(X_test))\n",
        "\n",
        "# Limited depth tree\n",
        "limited_tree = DecisionTreeClassifier(max_depth=3, random_state=42)\n",
        "limited_tree.fit(X_train, y_train)\n",
        "limited_acc = accuracy_score(y_test, limited_tree.predict(X_test))\n",
        "\n",
        "print(\"Fully grown tree accuracy:\", full_acc)\n",
        "print(\"Max depth=3 tree accuracy:\", limited_acc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lK3dDXp8D-U8",
        "outputId": "d023b794-ad40-4427-fa66-45162ec6ec33"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fully grown tree accuracy: 1.0\n",
            "Max depth=3 tree accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 8: Write a Python program to train a Decision Tree Regressor on the Boston Housing Dataset and print MSE and feature importances.\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load dataset\n",
        "boston = fetch_openml(name=\"boston\", version=1, as_frame=False)\n",
        "X, y = boston.data, boston.target\n",
        "\n",
        "# Split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train model\n",
        "model = DecisionTreeRegressor(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
        "print(\"Feature Importances:\", model.feature_importances_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6gB5OFg-EG5N",
        "outputId": "6d67edbc-3fc2-4ac4-b8a3-af967c489215"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Squared Error: 10.416078431372549\n",
            "Feature Importances: [5.12956739e-02 3.35270585e-03 5.81619171e-03 2.27940651e-06\n",
            " 2.71483790e-02 6.00326256e-01 1.36170630e-02 7.06881622e-02\n",
            " 1.94062297e-03 1.24638653e-02 1.10116089e-02 9.00872742e-03\n",
            " 1.93328464e-01]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Question 9: Write a Python program to tune max_depth and min_samples_split using GridSearchCV.\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "param_grid = {\n",
        "    \"max_depth\": [None, 3, 5, 10],\n",
        "    \"min_samples_split\": [2, 5, 10]\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(\n",
        "    DecisionTreeClassifier(random_state=42),\n",
        "    param_grid,\n",
        "    cv=5\n",
        ")\n",
        "\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "best_model = grid.best_estimator_\n",
        "accuracy = accuracy_score(y_test, best_model.predict(X_test))\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Model Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tcWBEqapENLn",
        "outputId": "5260fe8f-ec00-49b9-ae21-3577036afe2d"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'max_depth': None, 'min_samples_split': 2}\n",
            "Best Model Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Explain the full step-by-step process of building a Decision Tree for disease prediction and its business value.\n",
        "\n",
        "- First, missing values are handled by either removing incomplete records or imputing values using statistical measures such as mean, median, or mode. Medical datasets often benefit from domain-aware imputation.\n",
        "\n",
        "- Next, categorical features are encoded using techniques like Label Encoding or One-Hot Encoding so they can be processed by the model.\n",
        "\n",
        "- The dataset is then split into training and testing sets, and a Decision Tree model is trained using appropriate impurity criteria. Hyperparameters such as max_depth and min_samples_split are tuned using GridSearchCV to prevent overfitting.\n",
        "\n",
        "- Model performance is evaluated using metrics like accuracy, precision, recall, F1-score, and ROC-AUC to ensure reliability in healthcare decisions.\n",
        "\n",
        "- Business Value:\n",
        "This model enables early disease detection, improves clinical decision support, reduces diagnostic costs, and enhances patient outcomes by providing interpretable and actionable predictions for healthcare professionals."
      ],
      "metadata": {
        "id": "l2PmE27_EU1R"
      }
    }
  ]
}