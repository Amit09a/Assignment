{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. What is a Support Vector Machine (SVM)?\n",
        "\n",
        "- A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding an optimal hyperplane that separates data points of different classes with the maximum margin. The main objective of SVM is to maximize the distance between the hyperplane and the nearest data points from each class, which are called support vectors. By maximizing this margin, SVM improves generalization and reduces overfitting, making it effective in high-dimensional spaces.\n",
        "\n",
        "2. Difference Between Hard Margin and Soft Margin SVM\n",
        "\n",
        "- Hard Margin SVM assumes that the dataset is perfectly linearly separable and does not allow any misclassification. It strictly maximizes the margin while ensuring all data points are correctly classified. However, it is highly sensitive to noise and outliers. Soft Margin SVM, on the other hand, allows some misclassifications by introducing slack variables and a penalty parameter (C). This makes Soft Margin SVM more practical for real-world datasets where perfect separation is not possible.\n",
        "\n",
        "3. Mathematical Intuition Behind SVM\n",
        "\n",
        "- Mathematically, SVM attempts to find a hyperplane defined by\n",
        "ùë§\n",
        "‚ãÖ\n",
        "ùë•\n",
        "+\n",
        "ùëè\n",
        "=\n",
        "0\n",
        "- w‚ãÖx+b=0 that maximizes the margin between classes. The margin is defined as\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "‚à£‚à£w‚à£‚à£\n",
        "2\n",
        "\t‚Äã\n",
        "\n",
        ". The optimization problem minimizes\n",
        "1\n",
        "2\n",
        "‚à£\n",
        "‚à£\n",
        "ùë§\n",
        "‚à£\n",
        "‚à£\n",
        "2\n",
        "2\n",
        "1\n",
        "\t‚Äã\n",
        "\n",
        "‚à£‚à£w‚à£‚à£\n",
        "2\n",
        " subject to classification constraints. In soft margin SVM, slack variables and a penalty parameter C are added to allow misclassification while still maximizing the margin.\n",
        "\n",
        "4. Role of Lagrange Multipliers in SVM\n",
        "\n",
        "- Lagrange multipliers are used to convert the constrained optimization problem of SVM into a dual optimization problem. This allows the solution to be expressed in terms of dot products between data points. The dual formulation enables the use of kernel functions and simplifies computation, especially in high-dimensional spaces.\n",
        "\n",
        "5. What are Support Vectors?\n",
        "\n",
        "- Support vectors are the data points closest to the decision boundary (hyperplane). These points directly influence the position and orientation of the hyperplane. Removing non-support vectors does not change the model, but removing support vectors would alter the decision boundary significantly.\n",
        "\n",
        "6. What is Support Vector Classifier (SVC)?\n",
        "\n",
        "- Support Vector Classifier (SVC) is the classification implementation of SVM. It finds an optimal hyperplane that separates classes with maximum margin. It supports different kernels to handle both linear and non-linear classification problems.\n",
        "\n",
        "7. What is Support Vector Regressor (SVR)?\n",
        "\n",
        "- Support Vector Regressor (SVR) is the regression variant of SVM. Instead of maximizing the margin between classes, SVR attempts to fit a function within a specified error tolerance (epsilon). It minimizes prediction error while keeping the model as flat as possible.\n",
        "\n",
        "8. What is the Kernel Trick?\n",
        "\n",
        "- The Kernel Trick allows SVM to operate in high-dimensional feature spaces without explicitly computing transformations. It replaces dot products with kernel functions such as Linear, Polynomial, or RBF, enabling non-linear classification efficiently.\n",
        "\n",
        "9. Compare Linear, Polynomial, and RBF Kernel\n",
        "\n",
        "- The Linear kernel is used when data is linearly separable and is computationally efficient. The Polynomial kernel maps data into higher-degree polynomial feature space and is useful when relationships are non-linear but structured. The RBF (Radial Basis Function) kernel maps data into infinite-dimensional space and is highly flexible, making it effective for complex, non-linear patterns.\n",
        "\n",
        "10. Effect of C Parameter\n",
        "\n",
        "- The C parameter controls the trade-off between maximizing margin and minimizing classification error. A large C prioritizes minimizing misclassification, resulting in smaller margin and possible overfitting. A small C allows larger margin but tolerates more misclassification, improving generalization.\n",
        "\n",
        "11. Role of Gamma in RBF Kernel\n",
        "\n",
        "- Gamma defines how far the influence of a single training example reaches. A high gamma makes the decision boundary more complex and sensitive to individual points. A low gamma creates smoother decision boundaries.\n",
        "\n",
        "NA√èVE BAYES\n",
        "12. What is Na√Øve Bayes and Why is it Called Na√Øve?\n",
        "\n",
        "- Na√Øve Bayes is a probabilistic classification algorithm based on Bayes‚Äô Theorem. It assumes conditional independence between features given the class label. It is called ‚Äúna√Øve‚Äù because this independence assumption is often unrealistic, yet the classifier performs well in many real-world tasks.\n",
        "\n",
        "13. What is Bayes‚Äô Theorem?\n",
        "\n",
        "- Bayes‚Äô Theorem states:\n",
        "\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        "‚à£\n",
        "ùëã\n",
        ")\n",
        "=\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        "‚à£\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùê∂\n",
        ")\n",
        "ùëÉ\n",
        "(\n",
        "ùëã\n",
        ")\n",
        "P(C‚à£X)=\n",
        "P(X)\n",
        "P(X‚à£C)P(C)\n",
        "\t‚Äã\n",
        "\n",
        "\n",
        "- It calculates the posterior probability of a class given the features using prior probability and likelihood.\n",
        "\n",
        "14. Gaussian vs Multinomial vs Bernoulli Na√Øve Bayes\n",
        "\n",
        "- Gaussian Na√Øve Bayes assumes features follow a normal distribution and is used for continuous data. Multinomial Na√Øve Bayes is used for discrete count data such as word frequencies in text classification. Bernoulli Na√Øve Bayes is used for binary features representing presence or absence.\n",
        "\n",
        "15. When to Use Gaussian Na√Øve Bayes?\n",
        "\n",
        "- Gaussian Na√Øve Bayes is suitable when features are continuous and approximately normally distributed, such as medical or sensor data.\n",
        "\n",
        "16. Key Assumptions of Na√Øve Bayes\n",
        "\n",
        "- The main assumption is conditional independence of features given the class label. It also assumes consistent probability distributions depending on the variant used.\n",
        "\n",
        "17. Advantages and Disadvantages\n",
        "\n",
        "- Advantages include simplicity, speed, and effectiveness in high-dimensional problems. Disadvantages include unrealistic independence assumption and poor performance when features are highly correlated.\n",
        "\n",
        "18. Why Na√Øve Bayes is Good for Text Classification?\n",
        "\n",
        "- Na√Øve Bayes performs well in high-dimensional sparse data like text. Word occurrences are often treated as independent, making Multinomial NB particularly effective for spam detection and document classification.\n",
        "\n",
        "19. Compare SVM and Na√Øve Bayes\n",
        "\n",
        "- SVM is a discriminative model that maximizes margin and often achieves higher accuracy in complex datasets. Na√Øve Bayes is a generative model that estimates probabilities and is computationally faster. SVM performs better with complex decision boundaries, while Na√Øve Bayes is better for text classification and small datasets.\n",
        "\n",
        "20. How Laplace Smoothing Helps?\n",
        "\n",
        "- Laplace smoothing adds 1 to frequency counts to avoid zero probability issues when a feature does not appear in training data for a class."
      ],
      "metadata": {
        "id": "xCEM92bndDRR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import (accuracy_score, confusion_matrix, classification_report,\n",
        "                             mean_squared_error, mean_absolute_error,\n",
        "                             precision_recall_curve, roc_auc_score,\n",
        "                             log_loss)\n",
        "\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.feature_extraction.text import CountVectorizer"
      ],
      "metadata": {
        "id": "m37GWcddg2Rn"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#2. SVM Classifier on Iris Dataset\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "model = SVC(kernel='linear')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "y_pred = model.predict(X_test)\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F2ETJaH7g4Jr",
        "outputId": "f9727282-0ec1-4a5d-ef3f-e0b2b0aa744f"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear vs RBF Kernel (Wine Dataset)\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "linear = SVC(kernel='linear')\n",
        "rbf = SVC(kernel='rbf')\n",
        "\n",
        "linear.fit(X_train, y_train)\n",
        "rbf.fit(X_train, y_train)\n",
        "\n",
        "print(\"Linear Accuracy:\", accuracy_score(y_test, linear.predict(X_test)))\n",
        "print(\"RBF Accuracy:\", accuracy_score(y_test, rbf.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Z5uYT797hMpM",
        "outputId": "18ac1fc9-4854-4eb7-f5ef-6c8cb3f00c3c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Accuracy: 0.9814814814814815\n",
            "RBF Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Polynomial Kernel SVM\n",
        "poly = SVC(kernel='poly', degree=3)\n",
        "poly.fit(X_train, y_train)\n",
        "\n",
        "print(\"Polynomial Accuracy:\", accuracy_score(y_test, poly.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-DCvlyL8hnV8",
        "outputId": "9617563b-80aa-4459-85db-6c752352c3a4"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Polynomial Accuracy: 0.7592592592592593\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. SVM with Different C Values\n",
        "for c in [0.1, 1, 10]:\n",
        "    model = SVC(C=c, kernel='linear')\n",
        "    model.fit(X_train, y_train)\n",
        "    print(f\"C={c}, Accuracy:\", accuracy_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SenAcBHtho6e",
        "outputId": "b5c9f4d4-fe27-4814-d930-cc81ff807d77"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "C=0.1, Accuracy: 1.0\n",
            "C=1, Accuracy: 0.9814814814814815\n",
            "C=10, Accuracy: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Scaling Comparison\n",
        "scaler = StandardScaler()\n",
        "\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "model_unscaled = SVC()\n",
        "model_scaled = SVC()\n",
        "\n",
        "model_unscaled.fit(X_train, y_train)\n",
        "model_scaled.fit(X_train_scaled, y_train)\n",
        "\n",
        "print(\"Unscaled Accuracy:\", accuracy_score(y_test, model_unscaled.predict(X_test)))\n",
        "print(\"Scaled Accuracy:\", accuracy_score(y_test, model_scaled.predict(X_test_scaled)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgen3V1Ahsdw",
        "outputId": "c8e56bd0-e5ca-4d21-8ea3-75caf4336f92"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Unscaled Accuracy: 0.7592592592592593\n",
            "Scaled Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 7. GridSearchCV Hyperparameter Tuning\n",
        "param_grid = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.01, 0.1, 1],\n",
        "    'kernel': ['linear', 'rbf']\n",
        "}\n",
        "\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5)\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Best Score:\", grid.best_score_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M7uQ7VWVhxYx",
        "outputId": "0002223a-e4d4-41c2-91cf-a879b8b96420"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 0.1, 'gamma': 0.01, 'kernel': 'linear'}\n",
            "Best Score: 0.9276666666666668\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 8. SVM on Imbalanced Dataset\n",
        "from sklearn.datasets import make_classification\n",
        "\n",
        "X, y = make_classification(n_classes=2, weights=[0.9, 0.1],\n",
        "                           n_samples=1000, random_state=42)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "model = SVC(class_weight='balanced')\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy with class weighting:\",\n",
        "      accuracy_score(y_test, model.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DSx-t2qvh7tk",
        "outputId": "a5067a0e-1268-476b-afca-71d4e9041491"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy with class weighting: 0.9066666666666666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 9. Stratified K-Fold Cross Validation\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "scores = []\n",
        "\n",
        "for train_index, test_index in skf.split(X, y):\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    model = SVC()\n",
        "    model.fit(X_train, y_train)\n",
        "    scores.append(accuracy_score(y_test, model.predict(X_test)))\n",
        "\n",
        "print(\"Average Accuracy:\", np.mean(scores))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "32qpYsUFiScg",
        "outputId": "18ec77bb-6bc2-469a-fda7-81e0e6303a4e"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Accuracy: 0.929\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 10. SVM Regression (SVR)\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "\n",
        "housing = fetch_california_housing()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    housing.data, housing.target, test_size=0.3\n",
        ")\n",
        "\n",
        "svr = SVR()\n",
        "svr.fit(X_train, y_train)\n",
        "\n",
        "pred = svr.predict(X_test)\n",
        "\n",
        "print(\"MSE:\", mean_squared_error(y_test, pred))\n",
        "print(\"MAE:\", mean_absolute_error(y_test, pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T7dvAw9YiUzo",
        "outputId": "6dd31992-5070-41ec-f2bb-99e13660d290"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MSE: 1.3533388997978366\n",
            "MAE: 0.8642367230952747\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 11. Confusion Matrix Visualization\n",
        "cm = confusion_matrix(y_test, model.predict(X_test))\n",
        "sns.heatmap(cm, annot=True, fmt='d')\n",
        "plt.title(\"Confusion Matrix\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hj3YiAediY5B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 13. Gaussian Na√Øve Bayes (Breast Cancer)\n",
        "data = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    data.data, data.target, test_size=0.3\n",
        ")\n",
        "\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "pred = gnb.predict(X_test)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, pred))\n",
        "print(\"ROC-AUC:\", roc_auc_score(y_test, gnb.predict_proba(X_test)[:,1]))\n",
        "print(\"Log Loss:\", log_loss(y_test, gnb.predict_proba(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5jlD1ft2ivVC",
        "outputId": "3e312bfb-572a-4208-ee34-8fcdbd1a2bee"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9473684210526315\n",
            "ROC-AUC: 0.9867284937639911\n",
            "Log Loss: 0.5154800967208021\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 14. Multinomial Na√Øve Bayes (20 Newsgroups)\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "\n",
        "news = fetch_20newsgroups(subset='all')\n",
        "vectorizer = CountVectorizer()\n",
        "\n",
        "X = vectorizer.fit_transform(news.data)\n",
        "y = news.target\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)\n",
        "\n",
        "mnb = MultinomialNB()\n",
        "mnb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Accuracy:\", accuracy_score(y_test, mnb.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hp-rzlPfiylM",
        "outputId": "261895de-e03d-416d-c518-b20b1eff2b0d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.8461266360099045\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 15. Bernoulli Na√Øve Bayes (Binary Features)\n",
        "bnb = BernoulliNB()\n",
        "bnb.fit(X_train, y_train)\n",
        "\n",
        "print(\"Bernoulli Accuracy:\",\n",
        "      accuracy_score(y_test, bnb.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_TvezoIWi1xu",
        "outputId": "6e853276-f2bf-4c52-f389-db7a55dd94d7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bernoulli Accuracy: 0.6729748850371419\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 16. Laplace Smoothing Effect\n",
        "mnb_no_smooth = MultinomialNB(alpha=0)\n",
        "mnb_smooth = MultinomialNB(alpha=1)\n",
        "\n",
        "mnb_no_smooth.fit(X_train, y_train)\n",
        "mnb_smooth.fit(X_train, y_train)\n",
        "\n",
        "print(\"Without Smoothing:\",\n",
        "      accuracy_score(y_test, mnb_no_smooth.predict(X_test)))\n",
        "\n",
        "print(\"With Laplace Smoothing:\",\n",
        "      accuracy_score(y_test, mnb_smooth.predict(X_test)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p0-selhHi578",
        "outputId": "434b803a-9898-416f-afd2-88aa0634b81f"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/naive_bayes.py:898: RuntimeWarning: divide by zero encountered in log\n",
            "  self.feature_log_prob_ = np.log(smoothed_fc) - np.log(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without Smoothing: 0.16643084541917227\n",
            "With Laplace Smoothing: 0.8461266360099045\n"
          ]
        }
      ]
    }
  ]
}